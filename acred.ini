## Configuration File for acred(api)

[claimencoder]
logfile = claimencoder.log
app_name = claimencoder
semantic_encoder_dir = ../../../models/coinform/semantic_encoder/
port = 8071
# upon loading of the model, we test it using sts-dev
stsb_dev_path = data/evaluation/sts-dev.csv
# how many samples from sts-b to use for testing at launch?
# 1500 takes about 5 minutes on a decent server with only CPU
stsb_test_samples = 15

[claimneuralindex]
logfile = claimneuralindex.log
app_name = claimneuralindex
claim_embeddings_path = ../../../models/coinform/claim-embeddings/claim_embs.tsv
port = 8072
semencoder_url = http://localhost:8071/claimencoder

[worthinesschecker]
logfile = worthinesschecker.log
app_name = worthinesschecker
check_worthiness_model_path = ../../../models/coinform/check_worthiness_acc_0.95/
port = 8073
# upon loading of the model, we test it using clef19
clef19_test_worth_path = data/evaluation/clef19
clef_test_batch_size = 64
clef_test_worth_samples = 64

[stance]
fnc1_model_path = ../../../models/coinform/stance/saved_fnc1_classifier_acc_0.92
# upon loading of the model, we test it using fnc_test
fnc_test_bodies_path = data/evaluation/fnc1/competition_test_bodies.csv
fnc_test_stances_path = data/evaluation/fnc1/competition_test_stances.csv
# if specified: number of stances to predict. fnc1-dev has 25413 stances
#  on a laptop (no GPU) testing on 500 samples takes about 2.5 minutes
fnc_test_stances_samples = 10
fnc_test_batch_size = 64

[acred]
acred_factchecker_urls_path = factchecker_urls.txt
acred_pred_claim_search_url = http://localhost:8070/test/api/v1/claim/internal-search
acred_search_verify = True
acred_search_auth_user = testuser
acred_search_auth_pwrd = testpass
# thresholds
# default cut-off point between not-verifiable and the rest
cred_conf_threshold = 0.75
# optionally lose confidence when rating an article based on its website
# typically you want to only penalise articles more credible than a threshold
article_from_website_conf_factor = 0.9
article_from_website_cred_threshold_penalise = 0.2
# reduce similarity for certain stance labels
sentence_similarity_unrelated_factor = 0.8
sentence_similarity_discuss_factor = 0.85
# values 'nltk' or 'expert.ai'
sentence_extractor = nltk  
sentence_type = full_sentence  # and/or clause
# Needed for translating non-English documents into English
translation_service_url = https://example.com/text/translate
translation_service_key = secret
# Collections of DB sentences
relsents_in_colls =  fromClaimReviews,fromPrecrawled
relsents_search_url = http://localhost:8070/test/api/v1/search
relsents_search_verify = False
review_format = schema.org
worthiness_review = True
worthinesschecker_url = http://localhost:8073/worthinesschecker


[acredapi]
app_name = test
port = 8070
sentences_db_type = dict
sentences_extracted_db_csv = data/sentences-extractedFrom-Articles-40K.csv
sentences_from_ClaimReviews_db_csv = data/claims-from-ClaimReviews-45K.csv
claimReview_db_jsonl = data/claimReviews-pruned-45K.jsonl
neuralindex_url = http://localhost:8072/claimneuralindex
# currently the stance prediction service is co-located with the neural index
stance_pred_url = http://localhost:8072/claimneuralindex
# only apply stance detection if similarity is over this threshold # 0.75
stance_min_sim_threshold = 0.4

# Debug mode logs to stdout and enable Flask debugging
# Set to 0 for production!
debug = 1

# HTTPS must not be set to 0 for production server
https = 0
logfile = api.log

# SSL Certificate and Key, required for enabling HTTPS
ssl_crt = ssl.crt
ssl_key = ssl.key



